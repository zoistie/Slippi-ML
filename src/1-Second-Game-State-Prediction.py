# -*- coding: utf-8 -*-
"""AMATH445 Final Project Part 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11xidFQqzfTzw_mLCbHM58ePQFGzAkM69

**C   60 frame prediction model**
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset

df = pd.read_csv('/content/game_data.csv')
df = df.drop(columns = ['match_id','player','opponent'])

def prepare_60_frame_prediction_data_with_randomization(
    df, input_frames=60, output_frames=60, game_start_indicator=-123, keep_probability=0.1
):
    """
    Prepare data for 60-frame prediction with randomization.

    Args:
        df: DataFrame containing game data.
        input_frames: Number of past frames to use as input.
        output_frames: Number of frames to predict.
        game_start_indicator: Value that indicates the start of a new game.
        keep_probability: Probability of keeping each chunk of data.

    Returns:
        X, y: Input and output datasets.
    """
    X, y = [], []
    num_frames = len(df)

    # Identify game boundaries
    game_boundaries = df[df['frame_id'] == game_start_indicator].index.tolist()
    game_boundaries.append(num_frames)  # Add end of the last game as a boundary

    for start, end in zip(game_boundaries[:-1], game_boundaries[1:]):
        # Loop through frames within each game
        for i in range(start, end - input_frames - output_frames):
            # Apply randomization
            if torch.rand(1).item() < keep_probability:
                X.append(df.iloc[i : i + input_frames].values)  # Past frames
                y.append(df.iloc[i + input_frames : i + input_frames + output_frames].values)  # Future frames

    X_array = np.array(X)
    y_array = np.array(y)
    return torch.tensor(X_array, dtype=torch.float32), torch.tensor(y_array, dtype=torch.float32)

input_frames = 60
output_frames = 60
keep_probability = 0.1  # 10% chance to keep each chunk

X, y = prepare_60_frame_prediction_data_with_randomization(
    df, input_frames, output_frames, keep_probability=keep_probability
)

print(f"Number of input sequences: {len(X)}")
print(f"Shape of each input sequence: {X[0].shape}")
print(f"Shape of each output sequence: {y[0].shape}")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset
import torch

# Split the data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize scalers
scaler_X = StandardScaler()
scaler_y = StandardScaler()

# Fit scalers on training data only
X_train_normalized = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_val_normalized = scaler_X.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)

y_train_normalized = scaler_y.fit_transform(y_train.reshape(-1, y_train.shape[-1])).reshape(y_train.shape)
y_val_normalized = scaler_y.transform(y_val.reshape(-1, y_val.shape[-1])).reshape(y_val.shape)

# Convert normalized data to PyTorch tensors
X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_normalized, dtype=torch.float32)

X_val_tensor = torch.tensor(X_val_normalized, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val_normalized, dtype=torch.float32)

# Print dataset sizes
print(f"Training set size: {X_train_tensor.shape}")
print(f"Validation set size: {X_val_tensor.shape}")

# Create TensorDatasets
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

import torch.nn as nn

class BasicRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(BasicRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # RNN layer
        self.rnn = nn.RNN(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
        )

        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_size)

        # Fully connected output layer
        self.fc = nn.Linear(hidden_size, output_size)  # Output for each time step

    def forward(self, x):
        # Initialize hidden state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # Forward pass through RNN
        rnn_out, _ = self.rnn(x, h0)

        # Apply layer normalization to stabilize activations
        rnn_out = self.layer_norm(rnn_out)

        # Predict for each time step in the sequence
        output = self.fc(rnn_out)
        return output

import torch.nn as nn
import torch.optim as optim

# Hyperparameters
input_size = X.shape[2]  # Number of features per frame
hidden_size = 128        # Hidden layer size
num_layers = 2           # Number of RNN layers
output_size = y.shape[2]  # Number of features in the output
learning_rate = 0.001
num_epochs = 20

# Initialize model, loss function, and optimizer
model = BasicRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)
criterion = nn.MSELoss()  # Mean Squared Error for regression
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Move model to device (GPU if available)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

print(f"Model initialized and moved to device: {device}")

import matplotlib.pyplot as plt

def train_and_evaluate_with_metrics(
    model, criterion, optimizer, train_loader, val_loader, num_epochs, device
):
    train_losses, val_losses, metrics_per_epoch = [], [], []

    for epoch in range(1, num_epochs + 1):
        # Training phase
        model.train()
        train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            train_loss += loss.item()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        train_losses.append(train_loss / len(train_loader))

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
        val_losses.append(val_loss / len(val_loader))

        # Compute metrics every epoch
        metrics = evaluate_model(model, val_loader, device)
        metrics_per_epoch.append(metrics)

        # Print progress and metrics
        print(f"Epoch {epoch}/{num_epochs}")
        print(f"Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}")
        print(f"Metrics: {metrics}")

    # Plotting at the end
    plt.figure(figsize=(12, 6))

    # Plot losses
    plt.subplot(2, 1, 1)
    plt.plot(range(1, num_epochs + 1), train_losses, label="Train Loss")
    plt.plot(range(1, num_epochs + 1), val_losses, label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss")
    plt.legend()
    plt.grid()

    # Plot metrics (assuming scalar metrics like accuracy or F1-score)
    plt.subplot(2, 1, 2)
    plt.plot(range(1, num_epochs + 1), metrics_per_epoch, label="Metrics")
    plt.xlabel("Epochs")
    plt.ylabel("Metric")
    plt.title("Metrics per Epoch")
    plt.legend()
    plt.grid()

    plt.tight_layout()
    plt.show()

    return train_losses, val_losses, metrics_per_epoch

# Import necessary modules
import torch
import torch.nn as nn
import torch.optim as optim

# Hyperparameters
input_size = X.shape[2]  # Number of input features
hidden_size = 128        # Hidden layer size
num_layers = 2           # Number of RNN layers
output_size = y.shape[2]  # Number of output features
learning_rate = 0.001
num_epochs = 20

# Initialize the model
model = BasicRNN(
    input_size=input_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
    output_size=output_size,
    dropout=0.2
)

# Define the loss function and optimizer
criterion = nn.MSELoss()  # For regression
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define the metrics function (example: mean absolute error as a metric)
def evaluate_model(model, data_loader, device):
    model.eval()
    total_error = 0.0
    num_batches = 0

    with torch.no_grad():
        for X_batch, y_batch in data_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            total_error += torch.mean(torch.abs(outputs - y_batch)).item()
            num_batches += 1

    return total_error / num_batches

# Run the function
train_losses, val_losses, metrics_per_epoch = train_and_evaluate_with_metrics(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    train_loader=train_loader,
    val_loader=val_loader,
    num_epochs=num_epochs,
    device=device
)

"""Metrics = Mean Absolute Error"""