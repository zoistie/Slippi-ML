# -*- coding: utf-8 -*-
"""AMATH 445 Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17hQblGvhLQNvkNpm7dp5SjFHOH7trxD1
"""

import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset

df = pd.read_csv('/content/game_data.csv')
df = df.drop(columns = ['match_id','player','opponent'])

def split_games_for_stock_prediction(df):
    X = []
    y = []

    for _, group in df.groupby((df['frame_id'] == -123).cumsum()):  # Group by game
        frames = group.to_dict('records')  # Convert each game's frames into records (list of dicts)
        mid = len(frames) // 2  # Split the game into halves

        # Input features (first half)
        X.extend(frames[:mid])

        # Target stocks (second half)
        stocks_second_half = [frame['player_stocks'] for frame in frames[mid:]]
        y.extend(stocks_second_half)

    # Convert to DataFrame or Torch Tensor
    min_length = min(len(X), len(y))
    X = pd.DataFrame(X[:min_length])
    y = pd.Series(y[:min_length])

    return X, y

X, y = split_games_for_stock_prediction(df)

print(len(X))
print(len(y))

class BasicMLP(nn.Module):
    """
     A simple feedforward neural network with multiple hidden layers.
    """
    def __init__(self, input_size, hidden_size, output_size, n_hidden_layers):
        super(BasicMLP, self).__init__()
        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(inplace=True))
        self.hidden_layers = self._make_hidden_layers(n_hidden_layers, hidden_size)
        self.output_layer = nn.Linear(hidden_size, output_size)

    def _make_hidden_layers(self, n_hidden_layers, hidden_size):
        layers = []
        for _ in range(n_hidden_layers):
            layers += [nn.Linear(hidden_size, hidden_size), nn.ReLU(inplace=True)]
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.input_layer(x)
        x = self.hidden_layers(x)
        x = self.output_layer(x)
        return x

from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import torch

X, y = split_games_for_stock_prediction(df)

# Convert DataFrame to PyTorch tensors
X_tensor = torch.tensor(X.values, dtype=torch.float32)
y_tensor = torch.tensor(y.values, dtype=torch.float32)

# Split data into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)

# Create DataLoader for batching
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

import torch.nn as nn
import torch.optim as optim

# Hyperparameters
input_size = X_tensor.shape[1]  # Number of input features (columns in X)
hidden_size = 128  # Number of units in hidden layers
output_size = 1  # Predicting stock count, which is a single value per frame
n_hidden_layers = 3  # Number of hidden layers
learning_rate = 0.001
num_epochs = 20

# Initialize model, loss function, and optimizer
model = BasicMLP(input_size, hidden_size, output_size, n_hidden_layers)
criterion = nn.MSELoss()  # Mean Squared Error for regression (predicting continuous stock values)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

import matplotlib.pyplot as plt

# Training loop
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            # Forward pass
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_train_loss += loss.item()

        # Validation phase
        model.eval()
        running_val_loss = 0.0
        with torch.no_grad():
            for X_val_batch, y_val_batch in val_loader:
                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)

                # Forward pass
                val_predictions = model(X_val_batch)
                val_loss = criterion(val_predictions, y_val_batch)
                running_val_loss += val_loss.item()

        # Average losses for the epoch
        avg_train_loss = running_train_loss / len(train_loader)
        avg_val_loss = running_val_loss / len(val_loader)

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)

        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

    return train_losses, val_losses

# Plotting function
def plot_losses(train_losses, val_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss Per Epoch")
    plt.legend()
    plt.grid(True)
    plt.show()

# Call the training loop
train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)

# Plot the losses
plot_losses(train_losses, val_losses)

import matplotlib.pyplot as plt

# Example data (replace these with your actual loss values)
train_losses = [0.7919, 0.7911, 0.7925, 0.7924, 0.7920, 0.7917, 0.7916, 0.7916, 0.7916, 0.7915,
                0.7915, 0.7914, 0.7913, 0.7914, 0.7913, 0.7914, 0.7913, 0.7914, 0.7913, 0.7912]
val_losses = [0.7939, 0.7998, 0.7967, 0.7967, 0.7963, 0.7938, 0.7939, 0.7964, 0.7962, 0.7942,
              0.7943, 0.7940, 0.7945, 0.7945, 0.7950, 0.7938, 0.7938, 0.7938, 0.7938, 0.7976]

# Plotting
epochs = range(1, len(train_losses) + 1)

plt.figure(figsize=(10, 6))
plt.plot(epochs, train_losses, label='Train Loss', marker='o')
plt.plot(epochs, val_losses, label='Validation Loss', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()